@misc{wikiwerewolf,
  author = {{Wikipedia contributors}},
  title = {Mafia (party game)},
  year = {2024},
  url = {https://en.wikipedia.org/wiki/Mafia_(party_game)},
  note = {Accessed: 2024-05-06}
}

@inproceedings{stepputtis2023longhorizon,
  title = {Long-Horizon Dialogue Understanding for Role Identification in the Game of Avalon with Large Language Models},
  author = {Stepputtis, Simon and Campbell, Joseph and Xie, Yaqi and Qi, Zhengyang and Zhang, Wenxin Sharon and Wang, Ruiyi and Rangreji, Sanketh and Lewis, Charles Michael and Sycara, Katia P.},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  year = {2023},
  url = {https://openreview.net/forum?id=JKmsjKJ0Q8}
}



@online{bailisWerewolfArenaCase2024,
  title = {Werewolf {{Arena}}: {{A Case Study}} in {{LLM Evaluation}} via {{Social Deduction}}},
  shorttitle = {Werewolf {{Arena}}},
  author = {Bailis, Suma and Friedhoff, Jane and Chen, Feiyang},
  date = {2024-07-18},
  eprint = {2407.13943},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2407.13943},
  url = {http://arxiv.org/abs/2407.13943},
  urldate = {2025-05-01},
  abstract = {This paper introduces Werewolf Arena, a novel framework for evaluating large language models (LLMs) through the lens of the classic social deduction game, Werewolf. In Werewolf Arena, LLMs compete against each other, navigating the game's complex dynamics of deception, deduction, and persuasion. The framework introduces a dynamic turn-taking system based on bidding, mirroring real-world discussions where individuals strategically choose when to speak. We demonstrate the framework's utility through an arena-style tournament featuring Gemini and GPT models. Our results reveal distinct strengths and weaknesses in the models' strategic reasoning and communication. These findings highlight Werewolf Arena's potential as a challenging and scalable LLM benchmark.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/bjonnalagadda/Documents/Zotero Library/storage/M7MWQ9SW/Bailis et al. - 2024 - Werewolf Arena A Case Study in LLM Evaluation via Social Deduction.pdf;/Users/bjonnalagadda/Documents/Zotero Library/storage/M2FCPPQ9/2407.html}
}

@online{chiAMONGAGENTSEvaluatingLarge2024,
  title = {{{AMONGAGENTS}}: {{Evaluating Large Language Models}} in the {{Interactive Text-Based Social Deduction Game}}},
  shorttitle = {{{AMONGAGENTS}}},
  author = {Chi, Yizhou and Mao, Lingjun and Tang, Zineng},
  date = {2024-07-24},
  eprint = {2407.16521},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2407.16521},
  url = {http://arxiv.org/abs/2407.16521},
  urldate = {2025-05-01},
  abstract = {Strategic social deduction games serve as valuable testbeds for evaluating the understanding and inference skills of language models, offering crucial insights into social science, artificial intelligence, and strategic gaming. This paper focuses on creating proxies of human behavior in simulated environments, with Among Us utilized as a tool for studying simulated human behavior. The study introduces a text-based game environment, named AmongAgents, that mirrors the dynamics of Among Us. Players act as crew members aboard a spaceship, tasked with identifying impostors who are sabotaging the ship and eliminating the crew. Within this environment, the behavior of simulated language agents is analyzed. The experiments involve diverse game sequences featuring different configurations of Crewmates and Impostor personality archetypes. Our work demonstrates that state-of-the-art large language models (LLMs) can effectively grasp the game rules and make decisions based on the current context. This work aims to promote further exploration of LLMs in goal-oriented games with incomplete information and complex action spaces, as these settings offer valuable opportunities to assess language model performance in socially driven scenarios.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/bjonnalagadda/Documents/Zotero Library/storage/6M76EPC5/Chi et al. - 2024 - AMONGAGENTS Evaluating Large Language Models in the Interactive Text-Based Social Deduction Game.pdf;/Users/bjonnalagadda/Documents/Zotero Library/storage/TIPXEH2K/2407.html}
}

@online{choRoundTableInvestigatingGroup2024,
  title = {{{RoundTable}}: {{Investigating Group Decision-Making Mechanism}} in {{Multi-Agent Collaboration}}},
  shorttitle = {{{RoundTable}}},
  author = {Cho, Young-Min and Shu, Raphael and Das, Nilaksh and Alkhouli, Tamer and Lai, Yi-An and Cai, Jason and Sunkara, Monica and Zhang, Yi},
  date = {2024-11-11},
  eprint = {2411.07161},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2411.07161},
  url = {http://arxiv.org/abs/2411.07161},
  urldate = {2025-05-01},
  abstract = {This study investigates the efficacy of Multi-Agent Systems in eliciting cross-agent communication and enhancing collective intelligence through group decision-making in a decentralized setting. Unlike centralized mechanisms, where a fixed hierarchy governs social choice, decentralized group decision-making allows agents to engage in joint deliberation. Our research focuses on the dynamics of communication and decision-making within various social choice methods. By applying different voting rules in various environments, we find that moderate decision flexibility yields better outcomes. Additionally, exploring the linguistic features of agent-to-agent conversations reveals indicators of effective collaboration, offering insights into communication patterns that facilitate or hinder collaboration. Finally, we propose various methods for determining the optimal stopping point in multi-agent collaborations based on linguistic cues. Our findings contribute to a deeper understanding of how decentralized decision-making and group conversation shape multi-agent collaboration, with implications for the design of more effective MAS environments.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems},
  file = {/Users/bjonnalagadda/Documents/Zotero Library/storage/2QEJLXJQ/Cho et al. - 2024 - RoundTable Investigating Group Decision-Making Mechanism in Multi-Agent Collaboration.pdf;/Users/bjonnalagadda/Documents/Zotero Library/storage/2BS3UJC2/2411.html;/Users/bjonnalagadda/Documents/Zotero Library/storage/W99TDNID/2411.html}
}

@article{duLargeLanguageModels2024,
  title = {Large {{Language Models}} for {{Collective Problem-Solving}}: {{Insights}} into {{Group Consensus Decision-Making}}},
  shorttitle = {Large {{Language Models}} for {{Collective Problem-Solving}}},
  author = {Du, Yinuo and Rajivan, Prashanth and Gonzalez, Cleotilde},
  date = {2024},
  journaltitle = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  shortjournal = {Proc. Annu. Meet. Cogn. Sci. Soc.},
  volume = {46},
  number = {0},
  url = {https://escholarship.org/uc/item/6s060914},
  urldate = {2025-05-01},
  abstract = {Large Language models (LLM) exhibit human-like proficiency in various tasks such as translation, question answering, essay writing, and programming. Emerging research explores the use of LLMs in collective problem-solving endeavors, such as tasks where groups try to uncover clues through discussions. Although prior work has investigated individual problem-solving tasks, leveraging LLM-powered agents for group consensus and decision-making remains largely unexplored. This research addresses this gap by (1) proposing an algorithm to enable free-form conversation in groups of LLM agents, (2) creating metrics to evaluate the human-likeness of the generated dialogue and problem-solving performance, and (3) evaluating LLM agent groups against human groups using an open source dataset. Our results reveal that LLM groups outperform human groups in problem-solving tasks. LLM groups also show a greater improvement in scores after participating in free discussions. In particular, analyses indicate that LLM agent groups exhibit more disagreements, complex statements, and a propensity for positive statements compared to human groups. The results shed light on the potential of LLMs to facilitate collective reasoning and provide insight into the dynamics of group interactions involving synthetic LLM agents.},
  langid = {english},
  file = {/Users/bjonnalagadda/Documents/Zotero Library/storage/Z9NEHVSB/Du et al. - 2024 - Large Language Models for Collective Problem-Solving Insights into Group Consensus Decision-Making.pdf}
}

@online{laiWerewolfUsMultimodal2022,
  title = {Werewolf {{Among Us}}: {{A Multimodal Dataset}} for {{Modeling Persuasion Behaviors}} in {{Social Deduction Games}}},
  shorttitle = {Werewolf {{Among Us}}},
  author = {Lai, Bolin and Zhang, Hongxin and Liu, Miao and Pariani, Aryan and Ryan, Fiona and Jia, Wenqi and Hayati, Shirley Anugrah and Rehg, James M. and Yang, Diyi},
  date = {2022-12-16},
  eprint = {2212.08279},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.08279},
  url = {http://arxiv.org/abs/2212.08279},
  urldate = {2025-05-01},
  abstract = {Persuasion modeling is a key building block for conversational agents. Existing works in this direction are limited to analyzing textual dialogue corpus. We argue that visual signals also play an important role in understanding human persuasive behaviors. In this paper, we introduce the first multimodal dataset for modeling persuasion behaviors. Our dataset includes 199 dialogue transcriptions and videos captured in a multi-player social deduction game setting, 26,647 utterance level annotations of persuasion strategy, and game level annotations of deduction game outcomes. We provide extensive experiments to show how dialogue context and visual signals benefit persuasion strategy prediction. We also explore the generalization ability of language models for persuasion modeling and the role of persuasion strategies in predicting social deduction game outcomes. Our dataset, code, and models can be found at https://persuasion-deductiongame.socialai-data.org.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/bjonnalagadda/Documents/Zotero Library/storage/IFJ53BC6/Lai et al. - 2022 - Werewolf Among Us A Multimodal Dataset for Modeling Persuasion Behaviors in Social Deduction Games.pdf;/Users/bjonnalagadda/Documents/Zotero Library/storage/7J2CENPK/2212.html}
}

@article{piattiCooperateCollapseEmergence2024,
  title = {Cooperate or {{Collapse}}: {{Emergence}} of {{Sustainable Cooperation}} in a {{Society}} of {{LLM Agents}}},
  shorttitle = {Cooperate or {{Collapse}}},
  author = {Piatti, Giorgio and Jin, Zhijing and Kleiman-Weiner, Max and Schölkopf, Bernhard and Sachan, Mrinmaya and Mihalcea, Rada},
  date = {2024-12-16},
  journaltitle = {Advances in Neural Information Processing Systems},
  shortjournal = {Adv. Neural Inf. Process. Syst.},
  volume = {37},
  pages = {111715--111759},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/ca9567d8ef6b2ea2da0d7eed57b933ee-Abstract-Conference.html},
  urldate = {2025-05-01},
  langid = {english},
  file = {/Users/bjonnalagadda/Documents/Zotero Library/storage/4JXVP9TZ/Piatti et al. - 2024 - Cooperate or Collapse Emergence of Sustainable Cooperation in a Society of LLM Agents.pdf}
}

@online{xuLanguageAgentsReinforcement2024,
  title = {Language {{Agents}} with {{Reinforcement Learning}} for {{Strategic Play}} in the {{Werewolf Game}}},
  author = {Xu, Zelai and Yu, Chao and Fang, Fei and Wang, Yu and Wu, Yi},
  date = {2024-02-20},
  eprint = {2310.18940},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.18940},
  url = {http://arxiv.org/abs/2310.18940},
  urldate = {2025-05-01},
  abstract = {Agents built with large language models (LLMs) have shown great potential across a wide range of domains. However, in complex decision-making tasks, pure LLM-based agents tend to exhibit intrinsic bias in their choice of actions, which is inherited from the model's training data and results in suboptimal performance. To develop strategic language agents, i.e., agents that generate flexible language actions and possess strong decision-making abilities, we propose a novel framework that powers LLM-based agents with reinforcement learning (RL). We consider Werewolf, a popular social deduction game, as a challenging testbed that emphasizes versatile communication and strategic gameplay. To mitigate the intrinsic bias in language actions, our agents use an LLM to perform deductive reasoning and generate a diverse set of action candidates. Then an RL policy trained to optimize the decision-making ability chooses an action from the candidates to play in the game. Extensive experiments show that our agents overcome the intrinsic bias and outperform existing LLM-based agents in the Werewolf game. We also conduct human-agent experiments and find that our agents achieve human-level performance and demonstrate strong strategic play.},
  pubstate = {prepublished},
  version = {3},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/Users/bjonnalagadda/Documents/Zotero Library/storage/5YTRH99F/Xu et al. - 2024 - Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game.pdf;/Users/bjonnalagadda/Documents/Zotero Library/storage/S4Z59NXG/2310.html}
}

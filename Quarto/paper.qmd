---
title: "The Werewolf Among Us: Humans vs LLMs in Multi-Agent Games"
author:
  - name: Bhavana Jonnalagadda
    orcid: 0009-0009-1054-0995
    corresponding: true
    email: bhavana.jonnalagadda@colorado.edu
    affiliations:
      - name: University of Colorado Boulder
        id: 1
        city: Boulder
        state: CO
  - name: Riley Jones
    orcid: 0009-0001-8160-5508
    email: rjones@colorado.edu
    affiliations:
      - id: 1
keywords:
  - social deduction games
  - persuasion modeling
  - Werewolf Among Us dataset
  - large language models
  - multimodal analysis
abstract: |
  Abstract TODO
date: last-modified
citation:
  container-title: Human vs LLM Multi-Agent Games
---

::: {.content-visible when-format="html"}
::: {.column-margin}
![](Images/logo.png)
:::
:::

# Introduction

<!-- TODO: Broken for some reason?!?!? Fix -->
<!-- ::: {.content-hidden unless-format="latex"}
::: {.callout-tip}
## View Online
This PDF's content, the source code notebooks, along with live interactive versions of all figures (and more!) is available at the website version of this paper: [https://cuboulder-ds.github.io/CSCI-5423-Final](https://cuboulder-ds.github.io/CSCI-5423-Final)
:::
::: -->


Social deduction games like Werewolf offer a clear way to evaluate how agents deceive, persuade, and reason in group settings [@wikiwerewolf]. In these games, players operate with limited information and hidden identities, attempting to convince others while trying to uncover deception themselves. These dynamics mirror real-world challenges involving trust, negotiation, and manipulation. To explore how humans and large language models (LLMs) navigate such scenarios, we analyzed two key datasets: Werewolf Among Us [@laiWerewolfUsMultimodal2022], which features real human gameplay annotated with persuasion strategies, and Werewolf Arena [@bailisWerewolfArenaCase2024], a simulated environment in which LLM agents autonomously play the game. While both studies demonstrate that Werewolf elicits rich, strategic language, neither directly compares human and LLM behavior. Our project fills this gap. We analyzed transcripts from both datasets, aligning them by role, round, and persuasive strategy, and compared how humans and LLMs lie, persuade, and detect deception. By annotating all utterances using a consistent taxonomy of persuasive strategies, we expose key differences and similarities in how synthetic agents and humans handle adversarial group interactions.


## Related Work

### Multi-Agent LLMs

- Among us game [@chiAMONGAGENTSEvaluatingLarge2024]
- Collective problem solving [@duLargeLanguageModels2024]
  - "analyses indicate that LLM agent groups exhibit more disagreements, complex statements, and a propensity for positive statements compared to human groups"
- Govsim [@piattiCooperateCollapseEmergence2024]
  - "In GOVSIM, a society of AI agents must collectively balance exploiting a common resource with sustaining it for future use. This environment enables the study of how ethical considerations, strategic planning, and negotiation skills impact cooperative outcomes."
- All found similar themes
  - That LLMs are capable and good at understanding the rules
  - That they can cooperate and be sneaky

### LLMs and Werewolf

- Examination of improving werewolf by LLMs [@xuLanguageAgentsReinforcement2024]
  - "our agents use an LLM to perform deductive reasoning and generate a diverse set of action candidates. Then an RL policy trained to optimize the decision-making ability chooses an action from the candidates to play in the game. Extensive experiments show that our agents overcome the intrinsic bias and outperform existing LLM-based agents in the Werewolf game."
- Werewolf Arena [@bailisWerewolfArenaCase2024]
  - Used in this paper
- Explicitly discuss how none of the exisiting LLM+Werewolf papers examine the differences/compare from a human dataset

# Methods

## Data

### Werewolf Among Us Human Dataset

We used the *Werewolf Among Us* dataset \[@laiWerewolfUsMultimodal2022], containing annotated dialogues from over 150 real games of One Night Werewolf and Avalon. These games differ from classic Werewolf by having only one round of discussion and voting, not eliminating players during gameplay, and featuring specialized roles beyond Villager and Werewolf. The dataset includes detailed annotations of persuasion strategies for each utterance, such as accusations, defenses, and identity claims. Our analysis specifically utilized textual transcriptions and strategy annotations for direct comparison.

### Werewolf Arena (LLM Dataset)

The *Werewolf Arena* dataset \[@bailisWerewolfArenaCase2024] comprises simulated classic Werewolf games played by autonomous LLM agents. Unlike one-round human games, these simulations include multiple rounds alternating between night (secret actions) and day (open discussion). Each agent receives a role (Villager, Werewolf, Seer, Doctor) and interacts through tailored prompts generated via an LLM API.

We conducted simulations using five LLM models: GPT-4o, GPT-4.1, GPT-4o-mini, DeepSeek-Chat, and DeepSeek-Reasoner. Two configurations were tested:

* 8 players with 8 discussion rounds
* 10 players with 6 discussion rounds

We selected these settings to provide ample opportunity for villagers to coordinate and demonstrate persuasive behaviors.

Agents participated in gameplay through a graphical user interface (GUI), depicted in Figure 1. The GUI displays the game state, including player roles, actions, and current discussion rounds, enabling monitoring of the gameplay progression.

**Figure 1.** GUI of Werewolf Arena simulation environment.

![GUI of Werewolf Arena simulation](Images/WWA_GUI.PNG.png)



A central feature in Werewolf Arena is the dynamic turn-taking system implemented via a bidding mechanism. Rather than a fixed speaking order, agents bid for speaking turns based on urgency and strategic necessity, closely simulating real-world group discussions. Bidding levels range from passive observation to urgent direct responses:

* 0: Observe quietly
* 1: Share general thoughts
* 2: Contribute critical and specific information
* 3: Urgent need to speak
* 4: Respond directly after being addressed or accused

The highest bidder speaks next, with ties broken by prioritizing agents directly mentioned in preceding turns. This mechanism captures nuanced strategic communication decisions made by agents throughout the game.

Agents utilize specialized prompts reflecting their current role, memory state, and game context. The prompts guide strategic interactions, influencing agent decisions in voting, debating, and night actions. After generating dialogues through the LLM API, we manually annotated these interactions using the persuasion strategy categories from the human dataset.

## Analysis

Annotations were standardized across both datasets for direct comparative analysis. Our analyses explored frequency distributions of persuasion strategies, role-based comparisons (villager vs. werewolf), and strategic differences between human and LLM-generated dialogues.


# Results

{{< embed Data/EDA_Comparison.ipynb#fig-llm-winrounds >}}

{{< embed Data/EDA_Comparison.ipynb#fig-stratbyrole-hum >}}
{{< embed Data/EDA_Comparison.ipynb#fig-stratbyrole-llms >}}

# Discussion and Conclusion

Interpret findings, discuss limitations, and propose future work.

## Limitations

## Future Work

## Summary

Summarize contributions and insights from the project.

---

<!-- See https://quarto.org/docs/authoring/citations.html -->
# References

::: {#refs}
:::

---

# Project Contributions

**Bhavana Jonnalagadda**:

- Paper framework (Quarto) setup
- Github repo management
- EDA on LLM dataset
- Final comparison EDA and results analysis
- Results section
- Discussion and Conclusion section
- Abstract

**Riley Jones**:

- EDA on human dataset
- Werewolf Arena LLM simulation running and data aquisition
- Introduction section
- Methods section
